{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. import libraries\n",
    "#    settomg cuda\n",
    "# 2. define model\n",
    "# 3. load_weight\n",
    "# 4. predict PFT values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE global constant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We resampled the low-dose computed tomography (LDCT) scans to have 2.5mm isocubic voxels and cropped and padded the images to have an identical size of 180x178x140 (width, height, depth). This results in a sequence of coronal images, each with a width of 180 and height of 178, containing 140 slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 180,178\n",
    "slice_interval = 1\n",
    "NUM_CH = 1\n",
    "NUM_FRAMES = 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import SimpleITK as sitk\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting cuda environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"  # You can change this based on your GPU working environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.backend.image_data_format()\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the base code of i3d network in keras is quoted from keras version of i3d in \"https://github.com/dlpbc/keras-kinetics-i3d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import keras\n",
    "from keras.preprocessing import image\n",
    "from keras import backend as K\n",
    "\n",
    "#------\n",
    "from i3d_inception_last_global import Inception_Inflated3d as I3d\n",
    "from i3d_inception_last_global import conv3d_bn\n",
    "\n",
    "#------\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv3D\n",
    "from keras.layers import MaxPooling3D\n",
    "from keras.layers import AveragePooling3D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import GlobalAveragePooling3D\n",
    "\n",
    "\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "n_classes = 1\n",
    "\n",
    "\n",
    "###################################\n",
    "# we changed the last part of base model to global average pooling instead of 1*1*1 convolution\n",
    "###################################\n",
    "\n",
    "base_model = I3d(include_top=False,\n",
    "                weights=None,\n",
    "                input_shape=(NUM_FRAMES,178,180,NUM_CH),\n",
    "                endpoint_logit=False, # softmax will be applied\n",
    "                classes=n_classes\n",
    "                ) \n",
    "\n",
    "\n",
    "x = base_model.output\n",
    "x = Dense(500, activation = \"relu\")(x)\n",
    "prediction = Dense(1,activation = \"linear\" )(x)\n",
    "\n",
    "#----------------------------------\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions for image input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def preprocess_12bit(input_img):    # 12 bit full range \n",
    "    density_low = -1024\n",
    "    density_high = 3071\n",
    "  \n",
    "    output_img = (input_img-density_low) / (density_high-density_low+1)   \n",
    "    output_img[output_img < 0.] = 0.\n",
    "    output_img[output_img > 1.] = 1.\n",
    "    \n",
    "    return output_img\n",
    "\n",
    "def preprocess_lung(input_img):    # lung windowing\n",
    "\n",
    "    LEVEL = -700\n",
    "    WIDTH = 1500\n",
    "    density_low = LEVEL - WIDTH/2\n",
    "    density_high = LEVEL + WIDTH/2\n",
    "#     print(density_low, density_high)\n",
    "    \n",
    "    output_img = (input_img-density_low) / (density_high-density_low+1)   \n",
    "    output_img[output_img < 0.] = 0.\n",
    "    output_img[output_img > 1.] = 1.\n",
    "    \n",
    "    return output_img\n",
    "\n",
    "\n",
    "def preprocess_media(input_img):   \n",
    "\n",
    "    LEVEL = 50\n",
    "    WIDTH = 500\n",
    "    density_low = LEVEL - WIDTH/2\n",
    "    density_high = LEVEL + WIDTH/2\n",
    "\n",
    "    \n",
    "    output_img = (input_img-density_low) / (density_high-density_low+1)   \n",
    "    output_img[output_img < 0.] = 0.\n",
    "    output_img[output_img > 1.] = 1.\n",
    "    \n",
    "    return output_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path,augmentation = False, num_channel = NUM_CH):\n",
    "    img = sitk.GetArrayFromImage(sitk.ReadImage(path)) # 3d image를 불러 와야 함. (z, y, x)\n",
    "\n",
    "    if num_channel ==1:\n",
    "        img_12bit = preprocess_12bit(img) # (z,y,x)  \n",
    "        # we used the overall 12bit range of CT image\n",
    "        # you can change the windowing by change the preprocessing function and the the windowing level and width.\n",
    "        \n",
    "        img_tmp = np.moveaxis(img_12bit, [0,1], [1,0]) # (y, z, x)\n",
    "        img_tmp = img_tmp[:, ::-1, : ]# (y, -z, x)\n",
    "        \n",
    "        img_tmp = np.expand_dims(img_tmp, axis = -1)\n",
    "        img_out = img_tmp.copy()\n",
    "\n",
    "        if augmentation:\n",
    "            img_out = aug(img_out)\n",
    "            img_out = np.array(img_out)\n",
    "\n",
    "    return img_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. load_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEV1_weightpath = \n",
    "FVC_weifhtpath = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(FEV1_weightpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict PFT values from model image by image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_filename = \"####change the path as your environment\"\n",
    "\n",
    "img_test = load_image(_filename)\n",
    "\n",
    "test_predictions = model.predict(np.expand_dims(img_test, axis=0))\n",
    "\n",
    "test_predictions_value = model.predict(np.expand_dims(img_test, axis=0))[0][0] # predicted PFT values by the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict PFT values iteratively for the large dataset. \n",
    "# evaluate the performance of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from datetime import datetime\n",
    "now = datetime.now().strftime(\"%y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_prediction(_df, load_func, splitted_set, save_df = False):\n",
    "    \"\"\"\n",
    "    _df : pandas dataframe which contains columns with following information\n",
    "        [\"path_in_workspace\"] : path fo resampled CT image.\n",
    "        [\"FEV1\"] / [\"FVC\"] : measured values of FEV1 or FVC in liters.\n",
    "    load_func : we used previously defined function \"load_image\"\n",
    "    splitted_set : name of dataset during experiment --> \"training\"\"validation\"\"test\", which we used only for setting save filename\n",
    "    \n",
    "    save_df : use when you want the dataframe with test result\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    _idx = chosen_idx_of_weight\n",
    "\n",
    "    y= [ ]\n",
    "    score = [ ]\n",
    "    totScore=[]\n",
    "    test_true = []\n",
    "    test_predictions=[] # 예측값\n",
    "    test_predictions_value = []\n",
    "\n",
    "    for _idx, _row in tqdm.tqdm(_df.iterrows()):\n",
    "\n",
    "        _filename = _row[\"path_in_workspace\"]\n",
    "\n",
    "\n",
    "        img_test = load_func(_filename)\n",
    "        test_predictions.append(model.predict(np.expand_dims(img_test, axis=0)))\n",
    "\n",
    "        test_predictions_value.append(model.predict(np.expand_dims(img_test, axis=0))[0][0])\n",
    "        test_true.append(_row[\"FEV1\"])\n",
    "        \n",
    "    col_y_pred = \"modelpredicted(FEV1)\"\n",
    "    col_residual = \"residuals\"\n",
    "    col_resid_z = \"zscore_residuals\"\n",
    "    _df[col_y_pred] = test_predictions_value\n",
    "    _df[col_residual] = _df[col_y_pred]  - _df[col_y] \n",
    "    _df[col_resid_z] = zscore(_df[col_residual])\n",
    "    \n",
    "    if save_df:\n",
    "        save_path = os.path.join(PATH_trial,f\"df_{splitted_set}_result_({now}).xlsx\")\n",
    "        _df.to_excel(save_path)\n",
    "    return _df, col_y_pred, col_residual, col_resid_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Bland_Altman_by_sex_v2(target_df, col_measure1, col_measure2, check_criteria = 1.96, xlim_min = 0, xlim_max = 7):\n",
    "    import statsmodels.api as sm\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    target_measure_1 =col_measure1\n",
    "    target_measure_2 = col_measure2\n",
    "    target_str = target_measure_1.split(\"_\")[0]\n",
    "\n",
    "    df_target =target_df.copy() \n",
    "    ax_ylim = (df_target[target_measure_1] -df_target[target_measure_2]).abs().max() * 1.1\n",
    "#     ax_xlim_min = np.round(df_target[target_measure_1].min(), 2)\n",
    "#     ax_xlim_max = np.round(df_target[target_measure_1].max(), 2)\n",
    "    ax_xlim_min = xlim_min\n",
    "    ax_xlim_max = xlim_max\n",
    "#----------------------\n",
    "    M2_total = df_target[target_measure_1]\n",
    "    M1_total = df_target[target_measure_2]\n",
    "    \n",
    "    diffs_total = M1_total - M2_total \n",
    "    mean_diff_total = np.mean(diffs_total)\n",
    "    std_diff_total = np.std(diffs_total, axis = 0)\n",
    "    upper_total = mean_diff_total + check_criteria * std_diff_total\n",
    "    lower_total = mean_diff_total - check_criteria * std_diff_total\n",
    "    print(mean_diff_total,std_diff_total,upper_total  , lower_total)\n",
    "#-----------------\n",
    "#----------------------------------------------------------------\n",
    "    f, ax = plt.subplots(1,3, figsize = (21,5), sharex = \"row\", sharey = \"row\")   \n",
    "#         f.suptitle(\"Bland-Altman Plot\\n- First Visit 2018 - weak external validation set\", fontsize = 20)\n",
    "\n",
    "\n",
    "    for _i, (_sex, _color, _marker) in enumerate(zip([\"F\", \"M\"],colors, [\"+\", \"x\"])) :\n",
    "        df_sub = target_df[target_df[\"성별\"] == _sex]\n",
    "        if _sex == \"F\":\n",
    "            ax_title = \"Female\"\n",
    "        else:\n",
    "            ax_title = \"Male\"\n",
    "        # version 1\n",
    "    #     m1 =df_sub['FVC_MEAS ']\n",
    "    #     m2 = df_sub['modelpredicted(FVC_MEAS )']\n",
    "        # version 2\n",
    "        m2 =df_sub[target_measure_1]\n",
    "        m1 = df_sub[target_measure_2]    \n",
    "\n",
    "        sm.graphics.mean_diff_plot(m1, m2, ax = ax[_i],\n",
    "                                   sd_limit = check_criteria, \n",
    "                                scatter_kwds = {\"marker\":_marker, \n",
    "                                                 \"color\":_color, \n",
    "                                                 \"alpha\":0.7})\n",
    "        ax[_i].set_xlabel(f\"Mean {target_str} (L)\\n(Ground Truths + Predicted value)/2\", fontsize = 13)\n",
    "        ax[_i].set_title(ax_title, fontsize = 15)\n",
    "        ax[_i].set_ylim(-ax_ylim, ax_ylim)\n",
    "        ax[_i].set_xlim(ax_xlim_min, ax_xlim_max)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #----------------------------------------------------------------------\n",
    "        means = np.mean([m1, m2], axis=0)\n",
    "        diffs = m1 - m2\n",
    "        mean_diff = np.mean(diffs)\n",
    "        std_diff = np.std(diffs, axis=0)\n",
    "\n",
    "        ax[2].scatter(means, diffs, \n",
    "                     marker = _marker,\n",
    "                     color = _color, \n",
    "                     alpha = 0.7, label = _sex)\n",
    "\n",
    "\n",
    "    # draw line\n",
    "    ax[2].hlines(mean_diff_total,xmin =ax_xlim_min, xmax = ax_xlim_max,\n",
    "                color = \"k\", \n",
    "                linestyle = \"--\", linewidth = 1)\n",
    "    ax[2].hlines(mean_diff_total + check_criteria *std_diff_total,xmin =ax_xlim_min, xmax = ax_xlim_max,\n",
    "                color = \"k\",\n",
    "                 linestyle = \"--\", linewidth = 1)\n",
    "    ax[2].hlines(mean_diff_total - check_criteria *std_diff_total,xmin =ax_xlim_min, xmax = ax_xlim_max,\n",
    "                color = \"k\",\n",
    "                linestyle = \"--\", linewidth = 1)\n",
    "\n",
    "    # anootate \n",
    "    ax[2].annotate('mean diff:\\n{}'.format(np.round(mean_diff_total, 2)),\n",
    "                xy=(0.99, 0.5),\n",
    "                horizontalalignment='right',\n",
    "                verticalalignment='center',\n",
    "                fontsize=14,\n",
    "                xycoords='axes fraction')\n",
    "\n",
    "    ax[2].annotate('-SD{}: {}'.format(check_criteria, np.round(lower_total, 2)),\n",
    "            xy=(0.99, 0.07),\n",
    "            horizontalalignment='right',\n",
    "            verticalalignment='bottom',\n",
    "            fontsize=14,\n",
    "            xycoords='axes fraction')\n",
    "    ax[2].annotate('+SD{}: {}'.format(check_criteria, np.round(upper_total, 2)),\n",
    "            xy=(0.99, 0.92),\n",
    "            horizontalalignment='right',\n",
    "            fontsize=14,\n",
    "            xycoords='axes fraction')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ax[1].set_ylabel(\"\")  \n",
    "    ax[2].set_ylabel(\"\")\n",
    "    ax[0].set_ylabel(\"Differences\\n(Predicted values - Ground truth)\")\n",
    "\n",
    "    ax[2].tick_params(labelsize = 13)\n",
    "    ax[2].legend(loc = \"upper left\")\n",
    "    ax[2].set_xlabel(f\"Mean {target_str} (L)\\n(Ground Truth + Predicted values)/2\", fontsize = 13)\n",
    "    ax[2].set_ylim(-ax_ylim,ax_ylim)\n",
    "    ax[2].set_xlim(ax_xlim_min, ax_xlim_max)\n",
    "\n",
    "    f.tight_layout()   \n",
    "\n",
    "    return f, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score, max_error, median_absolute_error\n",
    "def get_RMSE(Y_TRUE, Y_PRED):\n",
    "    mse = np.average((Y_PRED - Y_TRUE)**2)\n",
    "    rmse = sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "def get_MAE(Y_TRUE, Y_PRED):\n",
    "    mae = np.average(abs(Y_PRED - Y_TRUE))\n",
    "    return mae\n",
    "\n",
    "def get_max_error(Y_TRUE, Y_PRED):\n",
    "    max_er = max_error(Y_PRED , Y_TRUE)\n",
    "    return max_er\n",
    "def get_uncentered_R_squared(Y_TRUE, Y_PRED):\n",
    "    SS_tot_uncentered = (Y_PRED**2).sum()\n",
    "    SS_res = ((Y_PRED - Y_TRUE)**2).sum()\n",
    "\n",
    "    R_squared_uncentered = 1- SS_res/SS_tot_uncentered\n",
    "    return R_squared_uncentered\n",
    "\n",
    "def get_CCC(Y_TRUE, Y_PRED):\n",
    "\n",
    "    x_bar = np.mean(Y_TRUE)\n",
    "    y_bar = np.mean(Y_PRED)\n",
    "\n",
    "    var_x = ((Y_TRUE - x_bar)**2).sum() / len(Y_TRUE)\n",
    "    var_y = ((Y_PRED - y_bar)**2).sum() / len(Y_PRED)\n",
    "\n",
    "    covar = ((Y_TRUE - x_bar)*(Y_PRED - y_bar)).sum()/ len(Y_TRUE)\n",
    "\n",
    "    rho_c= (2 * covar) / (var_x + var_y + (x_bar - y_bar)**2)\n",
    "    \n",
    "    return rho_c\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_evaluation_metrics(Y_TRUE, Y_PRED):\n",
    "#     print(tr_num, \"\\n\", _target_col)\n",
    "    rmse = get_RMSE(Y_TRUE, Y_PRED)\n",
    "    \n",
    "    mae_value = get_MAE(Y_TRUE, Y_PRED)\n",
    "    abs_max_error_value = get_max_error(Y_TRUE, Y_PRED)\n",
    "    median_abs_error_value = median_absolute_error(Y_TRUE, Y_PRED)\n",
    "    \n",
    "    r2_scipy_value = r2_score(Y_TRUE, Y_PRED)\n",
    "    \n",
    "#     correlation_matrix = np.corrcoef(Y_TRUE, Y_PRED)\n",
    "#     correlation_xy = correlation_matrix[0,1]\n",
    "#     r2_inverse = correlation_xy**2\n",
    "    r2_inverse = r2_score( Y_PRED, Y_TRUE)\n",
    "    resid = Y_PRED - Y_TRUE\n",
    "    r2_uncentered = get_uncentered_R_squared( Y_TRUE, Y_PRED)\n",
    "    Lin_concor = get_CCC(Y_TRUE, Y_PRED)\n",
    "    \n",
    "    print(\"RMSE: \", rmse)\n",
    "    print(\"MAE: \", mae_value)\n",
    "    print(\"R2_scipy: \", r2_scipy_value)\n",
    "    print(\"R2_scipy_inverse:\", r2_inverse)\n",
    "    print(\"R2_uncentered: \", r2_uncentered)\n",
    "    print(\"Lin Concordance correlation coefficient: \", Lin_concor)\n",
    "    \n",
    "    print(\"\\nmax resid: \", abs(resid).max())\n",
    "    print(\"mean resid: \", abs(resid).mean())\n",
    "    print(\"std resid: \", resid.std())\n",
    "    print(\"median resid: \", abs(resid).median())\n",
    "    \n",
    "    return rmse, mae_value, abs_max_error_value, median_abs_error_value, Lin_concor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target, col_y_pred, col_residual, col_resid_z = model_prediction(df_target, load_image,\"test\", save_df = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_true = df_target[\"FEV1\"]\n",
    "_pred = _true = df_target[\"modelpredicted(FEV1)\"]\n",
    "_rmse, _mae, _max_abser, _median_abser, _CC = get_evaluation_metrics(_true,_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting scatter\n",
    "import string\n",
    "alphabet_list = list(string.ascii_lowercase)\n",
    "#----------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "metric_font_style = {\"fontsize\": 18,\n",
    "                     \"transform\": \"ax.transAxes\"}\n",
    "bbox = dict(boxstyle=\"round,pad=0.5,rounding_size=0.2\", \n",
    "            fc=\"white\", \n",
    "            ec =\"lightgray\"\n",
    "           )\n",
    "\n",
    "_x, _y = _true,_pred\n",
    "_name = \"FEV1\"\n",
    "\n",
    "#==============================================================================\n",
    "_rmse, _mae, _max_abser, _median_abser, _CC = get_evaluation_metrics(_x,_y)\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 1, ncols = 1,figsize = (10, 10))\n",
    "label_text = f\"CCC  : {_CC:,.4f}\\nMAE  : {_mae:,.4f}\\nRMSE: {_rmse:,.4f}\"\n",
    "ax.annotate(label_text, (0.05, 0.85),\n",
    "                xycoords='axes fraction',\n",
    "        **metric_font_style, \n",
    "        bbox = bbox)\n",
    "\n",
    "ax.scatter(_x, _y, marker = \"o\", \n",
    "           alpha = 0.2,\n",
    "          s = 20)\n",
    "ax.plot([0, 150], [0, 150],\n",
    "     **line_styles, \n",
    "     )\n",
    "\n",
    "\n",
    "\n",
    "ax.tick_params(axis = \"both\",size = 10, labelsize = 18)\n",
    "ax.set_xlabel(\"Ground truth\", fontsize = 20)\n",
    "ax.set_ylabel(\"Predicted values\", fontsize = 20)\n",
    "\n",
    "\n",
    "if (i ==0) or (i==1):\n",
    "    ax.set_xlim(r1_min,r1_max)\n",
    "    ax.set_ylim(r1_min,r1_max)\n",
    "\n",
    "\n",
    "else:\n",
    "    ax.set_xlim(r2_min,r2_max)\n",
    "    ax.set_ylim(r2_min,r2_max)\n",
    "plt.savefig(\"put save path here\", \n",
    "            dpi = 300, bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
